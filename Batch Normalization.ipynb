{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a462a1f",
   "metadata": {},
   "source": [
    "#  1. Explain the concept of batch normalization in the context of Artificial Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8c7916",
   "metadata": {},
   "source": [
    "**Batch Normalization in Artificial Neural Networks:**\n",
    "\n",
    "Batch Normalization (BN) is a technique used to improve the training stability and speed of artificial neural networks. It operates by normalizing the inputs of each layer in a mini-batch of data, ensuring that the mean of the inputs is close to 0 and the standard deviation is close to 1. This normalization step is performed for each feature independently. Here's how it works:\n",
    "\n",
    "1. **Normalization Step:**\n",
    "   - For each feature in the input, calculate the mean and standard deviation over the mini-batch.\n",
    "   - Normalize the features using the calculated mean and standard deviation, so they have a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "   The normalization step helps in mitigating the vanishing or exploding gradient problem during backpropagation. It ensures that the gradients calculated during training are within a reasonable range, making the optimization process more stable.\n",
    "\n",
    "2. **Scaling and Shifting:**\n",
    "   - After normalization, the normalized features are scaled using a learnable parameter (gamma or γ) and shifted using another learnable parameter (beta or β).\n",
    "   - Gamma scales the normalized value, allowing the network to learn the appropriate scaling for each feature.\n",
    "   - Beta shifts the scaled and normalized value, enabling the network to learn the optimal mean for each feature.\n",
    "\n",
    "   Introducing learnable parameters allows the network to adapt and learn the optimal scale and translation for each feature, providing flexibility to the model.\n",
    "\n",
    "3. **Training and Inference:**\n",
    "   - During training, mean and standard deviation are calculated for each feature in the mini-batch. The normalized values are then scaled and shifted.\n",
    "   - During inference, the running averages of mean and standard deviation (accumulated during training) are used for normalization. This ensures consistent behavior during both training and prediction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eef055d",
   "metadata": {},
   "source": [
    "# 2 Describe the benefits of using batch normalization during trainingr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a183ad97",
   "metadata": {},
   "source": [
    "**Benefits of Batch Normalization:**\n",
    "1. **Stability:** Batch normalization reduces the risk of vanishing or exploding gradients, making the training process more stable.\n",
    "2. **Faster Convergence:** The stability introduced by batch normalization often allows for faster convergence, leading to quicker training times.\n",
    "3. **Regularization:** Batch normalization adds a slight noise to the activations, acting as a form of regularization and reducing the need for other regularization techniques like dropout.\n",
    "4. **Reduced Sensitivity to Initialization:** Batch normalization reduces sensitivity to the initial weights, making it easier to train deep networks.\n",
    "\n",
    "In summary, batch normalization normalizes the inputs of each layer, allowing for more stable and efficient training of neural networks. It is a widely used technique in modern deep learning architectures, contributing significantly to the success of training deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e283e0",
   "metadata": {},
   "source": [
    "# 3 Discuss the working principle of batch normalization, including the normalization step and the learnable parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4675345b",
   "metadata": {},
   "source": [
    "**Working Principle of Batch Normalization:**\n",
    "\n",
    "Batch Normalization (BN) is a technique used to normalize the inputs of each layer in a neural network. It operates on a mini-batch of data during training, normalizing the activations before they are passed to the next layer. This normalization step helps in mitigating issues like vanishing or exploding gradients and allows for more stable and faster training.\n",
    "\n",
    "**Normalization Step:**\n",
    "1. **Calculate Mean and Standard Deviation:** For each feature in the input, calculate the mean (\\(\\mu\\)) and standard deviation (\\(\\sigma\\)) over the mini-batch. This step computes the average and spread of the feature values in the current batch.\n",
    "2. **Normalize:** Normalize the features using the mean and standard deviation calculated in the previous step. For feature \\(x_i\\) in the mini-batch, the normalized value \\(\\hat{x}_i\\) is calculated as \\(\\hat{x}_i = \\frac{x_i - \\mu}{\\sigma + \\epsilon}\\), where \\(\\epsilon\\) is a small constant (usually added for numerical stability).\n",
    "3. **Scale and Shift:** After normalization, scale the normalized values using a learnable parameter (\\(\\gamma\\)) and shift them using another learnable parameter (\\(\\beta\\)). These parameters are specific to each feature in the layer.\n",
    "   - Scaled Value: \\(y_i = \\gamma \\hat{x}_i\\)\n",
    "   - Shifted Value: \\(z_i = y_i + \\beta\\)\n",
    "   - \\(y_i\\) represents the scaled and normalized value, and \\(z_i\\) is the final output of the batch normalization layer for feature \\(x_i\\).\n",
    "\n",
    "**Learnable Parameters:**\n",
    "- **Gamma (\\(\\gamma\\)):** Gamma is a learnable parameter that scales the normalized value. It allows the network to learn the optimal scaling for each feature. If \\(\\gamma\\) is close to 1, the normalized values are preserved; otherwise, they are scaled up or down.\n",
    "- **Beta (\\(\\beta\\)):** Beta is a learnable parameter that shifts the scaled and normalized value. It allows the network to learn the optimal mean for each feature. If \\(\\beta\\) is 0, the normalized values are centered around 0; otherwise, they are shifted.\n",
    "\n",
    "**Training and Inference:**\n",
    "- During training, mean and standard deviation are calculated for each feature in the mini-batch. The normalization, scaling, and shifting steps are applied using these batch-specific statistics.\n",
    "- During inference, the running averages of mean and standard deviation (accumulated during training) are used for normalization. This ensures consistent behavior during both training and prediction.\n",
    "\n",
    "By normalizing the inputs and allowing the network to learn the scaling and shifting parameters (\\(\\gamma\\) and \\(\\beta\\)), batch normalization ensures that the model trains faster, more stably, and often yields better generalization performance. It has become a standard practice in the design of deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be910ed",
   "metadata": {},
   "source": [
    "# Q2. Implementation:\n",
    "\n",
    "1. Choose a dataset of your choice (eg. MNIST, CIFAR-10) and preprocess it\n",
    "\n",
    "2. Implement a simple feedforward neural network using any deep learning framework/library (e.g., TensorFlow, PyTorch)\n",
    "\n",
    "3. Train the neural network on the chosen dataset without using batch normalization.\n",
    "\n",
    "4. Implement batch normalization layers in the neural network and train the model again.\n",
    "\n",
    "5. Compare the training and validation performance (eg, accuracy, loss) between the models with and without batch normalization.\n",
    "\n",
    "6. Discuss the impact of batch normalization on the training process and the performance of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8897a66e",
   "metadata": {},
   "source": [
    "# Step1:  Preprocessing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59efd05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize pixel values between 0 and 1\n",
    "y_train, y_test = to_categorical(y_train, 10), to_categorical(y_test, 10)  # One-hot encode labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33addfc",
   "metadata": {},
   "source": [
    "# Step 2: Implement a simple feedforward neural network without batch normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4333d73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 11s 5ms/step - loss: 0.2378 - accuracy: 0.9304 - val_loss: 0.1362 - val_accuracy: 0.9602\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.1023 - accuracy: 0.9679 - val_loss: 0.1184 - val_accuracy: 0.9636\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 12s 7ms/step - loss: 0.0708 - accuracy: 0.9779 - val_loss: 0.1054 - val_accuracy: 0.9659\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0539 - accuracy: 0.9826 - val_loss: 0.0751 - val_accuracy: 0.9777\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0430 - accuracy: 0.9863 - val_loss: 0.0877 - val_accuracy: 0.9739\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0349 - accuracy: 0.9882 - val_loss: 0.0801 - val_accuracy: 0.9781\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0299 - accuracy: 0.9901 - val_loss: 0.0856 - val_accuracy: 0.9774\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0245 - accuracy: 0.9916 - val_loss: 0.0957 - val_accuracy: 0.9754\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0199 - accuracy: 0.9932 - val_loss: 0.1089 - val_accuracy: 0.9748\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 12s 6ms/step - loss: 0.0204 - accuracy: 0.9930 - val_loss: 0.0899 - val_accuracy: 0.9780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1b64e912550>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense\n",
    "\n",
    "# Model without batch normalization\n",
    "model_without_bn = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_without_bn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_without_bn.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32f4d0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 17s 8ms/step - loss: 0.2499 - accuracy: 0.9253 - val_loss: 0.1213 - val_accuracy: 0.9630\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 14s 8ms/step - loss: 0.1245 - accuracy: 0.9618 - val_loss: 0.0895 - val_accuracy: 0.9732\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0972 - accuracy: 0.9699 - val_loss: 0.0879 - val_accuracy: 0.9708\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0786 - accuracy: 0.9752 - val_loss: 0.0905 - val_accuracy: 0.9721\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0658 - accuracy: 0.9785 - val_loss: 0.0804 - val_accuracy: 0.9744\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 16s 8ms/step - loss: 0.0585 - accuracy: 0.9815 - val_loss: 0.0819 - val_accuracy: 0.9745\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 15s 8ms/step - loss: 0.0514 - accuracy: 0.9834 - val_loss: 0.0760 - val_accuracy: 0.9779\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0445 - accuracy: 0.9855 - val_loss: 0.0830 - val_accuracy: 0.9758\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0430 - accuracy: 0.9855 - val_loss: 0.0715 - val_accuracy: 0.9795\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 14s 7ms/step - loss: 0.0395 - accuracy: 0.9876 - val_loss: 0.0781 - val_accuracy: 0.9765\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1b677e32a50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Model with batch normalization\n",
    "model_with_bn = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),  # Add batch normalization layer\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),  # Add batch normalization layer\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model_with_bn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_with_bn.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3b939a",
   "metadata": {},
   "source": [
    "## Compare and discuss the impact of batch normalization:\n",
    "\n",
    "After training both models, you can compare their training and validation performance metrics, such as accuracy and loss. Typically, you'll find that the model with batch normalization converges faster and achieves better generalization due to the stabilizing effect of batch normalization.\n",
    "\n",
    "Batch normalization helps in smoother and quicker convergence by maintaining stable activations and gradients throughout the training process. It also reduces the dependence on initialization choices and acts as a regularizer, potentially reducing the need for other regularization techniques. Additionally, batch normalization can enable the use of higher learning rates, further speeding up the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064397c2",
   "metadata": {},
   "source": [
    "# epsilon is small constant (configurable as part of the constructor arguments)\n",
    "# gamma is a learned scaling factor (initialized as 1), which can be disabled by passing scale=False to the constructor.\n",
    "# beta is a learned offset factor (initialized as 0), which can be disabled by passing center=False to the constructor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd97f7e4",
   "metadata": {},
   "source": [
    "# Q3. Experimental analysis\n",
    "1) Experiment with different batch sizes and observe the effect on the training dynamics and model\n",
    "performancer\n",
    "2) Discuss the advantages and potential limitations of batch normalization in improving the training of\n",
    "neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14eb678d",
   "metadata": {},
   "source": [
    "**Experimentation and Analysis:**\n",
    "\n",
    "**1. Experimenting with Different Batch Sizes:**\n",
    "   - Train the same neural network architecture with various batch sizes (e.g., 16, 32, 64, 128).\n",
    "   - Observe the training dynamics, including the training time, convergence speed, and generalization performance on validation and test datasets.\n",
    "   - Compare the training loss and accuracy curves for different batch sizes.\n",
    "   - Analyze how different batch sizes impact the model's ability to generalize to unseen data.\n",
    "\n",
    "**2. Advantages and Limitations of Batch Normalization:**\n",
    "\n",
    "**Advantages:**\n",
    "1. **Stabilizes Training:** Batch normalization helps stabilize the training process by reducing internal covariate shift. It ensures consistent activations and gradients, allowing for faster convergence and smoother training dynamics.\n",
    "2. **Enables Higher Learning Rates:** Batch normalization often allows the use of higher learning rates without causing divergence or overshooting the optimal weights, speeding up the convergence process.\n",
    "3. **Acts as a Regularizer:** Batch normalization introduces slight noise during training, acting as a form of regularization. This can reduce the need for other regularization techniques like dropout, potentially simplifying the model architecture.\n",
    "4. **Reduces Sensitivity to Initialization:** Batch normalization mitigates the sensitivity of the network to the choice of initial weights, making it easier to train deep networks.\n",
    "5. **Improves Generalization:** By providing a stable training process, batch normalization can lead to better generalization performance on unseen data, improving the model's ability to handle real-world scenarios.\n",
    "\n",
    "**Limitations:**\n",
    "1. **Not Suitable for Small Batch Sizes:** Batch normalization requires computing batch statistics (mean and variance) for normalization. In small batches, these statistics might not accurately represent the entire dataset, leading to suboptimal normalization.\n",
    "2. **Computational Overhead:** Batch normalization introduces additional computations during both training and inference, which can impact the overall computational efficiency, especially on resource-constrained devices.\n",
    "3. **Sequence-Dependent Data:** For sequential data like time series or natural language processing, batch normalization might not work well because the concept of batches is not well-defined, making it challenging to apply batch normalization effectively.\n",
    "4. **Dependency on Batch Order:** The order of data samples in a batch can impact the normalization statistics, potentially leading to variability in the training process.\n",
    "\n",
    "**Experimental Analysis:**\n",
    "- Compare the training dynamics (speed, stability) across different batch sizes.\n",
    "- Evaluate the generalization performance on validation and test datasets for each batch size.\n",
    "- Observe any trade-offs between training speed and model accuracy concerning batch size.\n",
    "- Consider the computational resources available and select a batch size that balances training efficiency and model performance.\n",
    "\n",
    "By conducting these experiments and analyzing the results, you can gain insights into the impact of batch size on training dynamics and understand the advantages and limitations of batch normalization in improving neural network training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f5dd91",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "936969c8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
